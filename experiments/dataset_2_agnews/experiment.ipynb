{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.8.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-00",
   "metadata": {},
   "source": [
    "# Phase 1 Experiment — Dataset 2: AG News\n",
    "\n",
    "**Task:** Topic Classification  \n",
    "**Sequence Type:** MEDIUM (~200 tokens)  \n",
    "**Classes:** 4 (World / Sports / Business / Sci-Tech)  \n",
    "**Source:** HuggingFace `ag_news`\n",
    "\n",
    "**Goal:** Compare all 5 PE methods on medium-length topic classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from PE.sinusoidal_pe import SinusoidalPositionalEncoding\n",
    "from PE.binary_pe    import BinaryPositionalEncoding\n",
    "from PE.rope         import RoPEPositionalEncoding\n",
    "from PE.learned_pe   import LearnedPositionalEncoding\n",
    "from PE.dape         import DAPEPositionalEncoding\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    dataset_name = 'ag_news'\n",
    "    text_col     = 'text'\n",
    "    label_col    = 'label'\n",
    "    n_classes    = 4\n",
    "    max_seq_len  = 256        # AG News articles are medium length\n",
    "    vocab_size   = 20000\n",
    "    max_train    = 30000      # subset for speed\n",
    "    max_val      = 5000\n",
    "\n",
    "    d_model  = 128\n",
    "    n_heads  = 4\n",
    "    n_layers = 3\n",
    "    d_ff     = 256\n",
    "    dropout  = 0.1\n",
    "\n",
    "    batch_size = 64\n",
    "    lr         = 1e-3\n",
    "    epochs     = 10\n",
    "    seed       = 42\n",
    "\n",
    "cfg = Config()\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "print('Config loaded. n_classes =', cfg.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading AG News...')\n",
    "raw = load_dataset('ag_news')\n",
    "print(raw)\n",
    "\n",
    "train_texts  = raw['train'][cfg.text_col][:cfg.max_train]\n",
    "train_labels = raw['train'][cfg.label_col][:cfg.max_train]\n",
    "val_texts    = raw['test'][cfg.text_col][:cfg.max_val]\n",
    "val_labels   = raw['test'][cfg.label_col][:cfg.max_val]\n",
    "\n",
    "print(f'Train: {len(train_texts)} | Val: {len(val_texts)}')\n",
    "print(f'Label names: {raw[\"train\"].features[\"label\"].names}')\n",
    "\n",
    "counter = Counter()\n",
    "for text in train_texts:\n",
    "    counter.update(text.lower().split())\n",
    "\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "for word, _ in counter.most_common(cfg.vocab_size - 2):\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "print(f'Vocabulary size: {len(vocab)}')\n",
    "\n",
    "lengths = [len(t.split()) for t in train_texts]\n",
    "print(f'Seq length — mean: {np.mean(lengths):.1f} | max: {max(lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, vocab, max_len):\n",
    "    tokens = text.lower().split()[:max_len]\n",
    "    ids = [vocab.get(t, 1) for t in tokens]\n",
    "    ids += [0] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.X = torch.tensor([tokenize(t, vocab, max_len) for t in texts], dtype=torch.long)\n",
    "        self.y = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = TextDataset(train_texts, train_labels, vocab, cfg.max_seq_len)\n",
    "val_ds   = TextDataset(val_texts,   val_labels,   vocab, cfg.max_seq_len)\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False)\n",
    "print(f'Train batches: {len(train_loader)} | Val batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads; self.d_k = d_model // n_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model); self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model); self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        Q = self.W_q(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        out = torch.matmul(self.dropout(F.softmax(scores, dim=-1)), V)\n",
    "        return self.W_o(out.transpose(1, 2).contiguous().view(B, T, D))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model))\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model); self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x + self.drop(self.attn(x)))\n",
    "        return self.ln2(x + self.drop(self.ff(x)))\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, d_model, n_heads, n_layers, d_ff, max_seq_len, pe_class, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe = pe_class(d_model, max_seq_len, dropout)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, n_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.pe(self.embedding(x))\n",
    "        for b in self.blocks: x = b(x)\n",
    "        return self.head(self.norm(x.mean(dim=1)))\n",
    "\n",
    "print('Model classes defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train(); total_loss, correct, total = 0, 0, 0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(X), y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        correct += (model(X).argmax(-1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval(); all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            preds = model(X.to(device)).argmax(-1).cpu()\n",
    "            all_preds.extend(preds.tolist()); all_labels.extend(y.tolist())\n",
    "    return accuracy_score(all_labels, all_preds), f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print('Training/eval functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-07",
   "metadata": {},
   "outputs": [],
   "source": [
    "PE_METHODS = {\n",
    "    'sinusoidal': SinusoidalPositionalEncoding,\n",
    "    'binary':     BinaryPositionalEncoding,\n",
    "    'rope':       RoPEPositionalEncoding,\n",
    "    'learned':    LearnedPositionalEncoding,\n",
    "    'dape':       DAPEPositionalEncoding,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for pe_name, pe_class in PE_METHODS.items():\n",
    "    print(f'\\n=== {pe_name.upper()} PE ===')\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    model = TransformerClassifier(\n",
    "        len(vocab), cfg.n_classes, cfg.d_model, cfg.n_heads,\n",
    "        cfg.n_layers, cfg.d_ff, cfg.max_seq_len, pe_class, cfg.dropout\n",
    "    ).to(device)\n",
    "    print(f'  Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "    best_acc, best_f1, t0 = 0, 0, time.time()\n",
    "    for epoch in range(cfg.epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_acc, val_f1 = evaluate(model, val_loader)\n",
    "        scheduler.step()\n",
    "        if val_acc > best_acc: best_acc, best_f1 = val_acc, val_f1\n",
    "        print(f'  Ep {epoch+1:02d} | loss {train_loss:.4f} | train {train_acc:.4f} | val_acc {val_acc:.4f} | val_f1 {val_f1:.4f}')\n",
    "    elapsed = time.time() - t0\n",
    "    results[pe_name] = {'accuracy': best_acc, 'f1': best_f1, 'time_s': elapsed}\n",
    "    print(f'  Done in {elapsed:.1f}s — best val acc: {best_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('PHASE 1 RESULTS — AG News (MEDIUM sequences, 4-class topic)')\n",
    "print('='*60)\n",
    "print(f'{\"PE Method\":<15} {\"Accuracy\":>10} {\"F1\":>10} {\"Time (s)\":>10}')\n",
    "print('-'*60)\n",
    "best_acc_val = max(v['accuracy'] for v in results.values())\n",
    "for pe_name, m in results.items():\n",
    "    marker = ' <-- BEST' if m['accuracy'] == best_acc_val else ''\n",
    "    print(f'{pe_name:<15} {m[\"accuracy\"]:>10.4f} {m[\"f1\"]:>10.4f} {m[\"time_s\"]:>10.1f}{marker}')\n",
    "print('='*60)\n",
    "\n",
    "names = list(results.keys())\n",
    "accs  = [results[n]['accuracy'] for n in names]\n",
    "f1s   = [results[n]['f1']       for n in names]\n",
    "times = [results[n]['time_s']   for n in names]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "colors = ['#4C72B0', '#DD8452', '#55A868', '#C44E52', '#8172B2']\n",
    "axes[0].bar(names, accs, color=colors); axes[0].set_title('Accuracy — AG News'); axes[0].tick_params(axis='x', rotation=15)\n",
    "axes[1].bar(names, f1s,  color=colors); axes[1].set_title('F1 Score — AG News'); axes[1].tick_params(axis='x', rotation=15)\n",
    "axes[2].bar(names, times,color=colors); axes[2].set_title('Training Time (s)');   axes[2].tick_params(axis='x', rotation=15)\n",
    "plt.suptitle('Phase 1: PE Comparison on AG News (MEDIUM sequences)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results_agnews.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: results_agnews.png')"
   ]
  }
 ]
}
