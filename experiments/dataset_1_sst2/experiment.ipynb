{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.8.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-00",
   "metadata": {},
   "source": [
    "# Phase 1 Experiment — Dataset 1: SST-2\n",
    "\n",
    "**Task:** Binary Sentiment Analysis  \n",
    "**Sequence Type:** SHORT (~50 tokens)  \n",
    "**Classes:** 2 (positive / negative)  \n",
    "**Source:** HuggingFace `glue/sst2`\n",
    "\n",
    "**Goal:** Compare all 5 PE methods on short-sequence sentiment classification.\n",
    "\n",
    "| PE Method     | Expected Strength |\n",
    "|---------------|------------------|\n",
    "| Sinusoidal    | Solid baseline    |\n",
    "| Binary        | May excel (short) |\n",
    "| RoPE          | Relative pos info |\n",
    "| Learned       | Task-specific     |\n",
    "| DAPE          | Adaptive          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "# !pip install datasets transformers torch scikit-learn tqdm matplotlib\n",
    "\n",
    "import os, sys, math, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path so we can import from PE/\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from PE.sinusoidal_pe import SinusoidalPositionalEncoding\n",
    "from PE.binary_pe    import BinaryPositionalEncoding\n",
    "from PE.rope         import RoPEPositionalEncoding\n",
    "from PE.learned_pe   import LearnedPositionalEncoding\n",
    "from PE.dape         import DAPEPositionalEncoding\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'Project root: {PROJECT_ROOT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Config ────────────────────────────────────────────────────────────────\n",
    "class Config:\n",
    "    # Dataset\n",
    "    dataset_name  = 'sst2'\n",
    "    dataset_split = 'glue'\n",
    "    text_col      = 'sentence'\n",
    "    label_col     = 'label'\n",
    "    n_classes     = 2\n",
    "    max_seq_len   = 128        # SST-2 sentences are short\n",
    "    vocab_size    = 10000\n",
    "    max_train     = 20000      # use subset for speed; set None for full\n",
    "    max_val       = 3000\n",
    "\n",
    "    # Model (identical for all PE methods — fair comparison)\n",
    "    d_model  = 128\n",
    "    n_heads  = 4\n",
    "    n_layers = 3\n",
    "    d_ff     = 256\n",
    "    dropout  = 0.1\n",
    "\n",
    "    # Training\n",
    "    batch_size = 64\n",
    "    lr         = 1e-3\n",
    "    epochs     = 10\n",
    "    seed       = 42\n",
    "\n",
    "cfg = Config()\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "print('Config loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load SST-2 ────────────────────────────────────────────────────────────\n",
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading SST-2...')\n",
    "raw = load_dataset('glue', 'sst2')\n",
    "print(raw)\n",
    "\n",
    "train_texts  = raw['train'][cfg.text_col][:cfg.max_train]\n",
    "train_labels = raw['train'][cfg.label_col][:cfg.max_train]\n",
    "val_texts    = raw['validation'][cfg.text_col][:cfg.max_val]\n",
    "val_labels   = raw['validation'][cfg.label_col][:cfg.max_val]\n",
    "\n",
    "print(f'Train: {len(train_texts)} | Val: {len(val_texts)}')\n",
    "print(f'Example: \"{train_texts[0]}\" -> label {train_labels[0]}')\n",
    "\n",
    "# Build vocabulary from training set\n",
    "counter = Counter()\n",
    "for text in train_texts:\n",
    "    counter.update(text.lower().split())\n",
    "\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "for word, _ in counter.most_common(cfg.vocab_size - 2):\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "print(f'Vocabulary size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Dataset class + DataLoaders ───────────────────────────────────────────\n",
    "def tokenize(text, vocab, max_len):\n",
    "    tokens = text.lower().split()[:max_len]\n",
    "    ids = [vocab.get(t, 1) for t in tokens]  # 1 = UNK\n",
    "    ids += [0] * (max_len - len(ids))         # 0 = PAD\n",
    "    return ids\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.X = torch.tensor([tokenize(t, vocab, max_len) for t in texts], dtype=torch.long)\n",
    "        self.y = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = TextDataset(train_texts, train_labels, vocab, cfg.max_seq_len)\n",
    "val_ds   = TextDataset(val_texts,   val_labels,   vocab, cfg.max_seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "print(f'Train batches: {len(train_loader)} | Val batches: {len(val_loader)}')\n",
    "\n",
    "# Sequence length distribution\n",
    "lengths = [len(t.split()) for t in train_texts]\n",
    "print(f'Seq length — mean: {np.mean(lengths):.1f} | max: {max(lengths)} | min: {min(lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Model (shared across all PE methods) ──────────────────────────────────\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        Q = self.W_q(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn = self.dropout(F.softmax(scores, dim=-1))\n",
    "        out = torch.matmul(attn, V).transpose(1, 2).contiguous().view(B, T, D)\n",
    "        return self.W_o(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff   = FeedForward(d_model, d_ff, dropout)\n",
    "        self.ln1  = nn.LayerNorm(d_model)\n",
    "        self.ln2  = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x + self.drop(self.attn(x)))\n",
    "        x = self.ln2(x + self.drop(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, n_classes, d_model, n_heads, n_layers, d_ff,\n",
    "                 max_seq_len, pe_class, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pe        = pe_class(d_model, max_seq_len, dropout)\n",
    "        self.blocks    = nn.ModuleList([TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "                                        for _ in range(n_layers)])\n",
    "        self.norm      = nn.LayerNorm(d_model)\n",
    "        self.head      = nn.Linear(d_model, 1 if n_classes == 2 else n_classes)\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)     # (B, T, D)\n",
    "        x = self.pe(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x.mean(dim=1))  # global average pool\n",
    "        return self.head(x)\n",
    "\n",
    "print('Model classes defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Training + Evaluation Functions ──────────────────────────────────────\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X).squeeze(-1)\n",
    "        loss = criterion(logits, y.float() if cfg.n_classes == 2 else y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        if cfg.n_classes == 2:\n",
    "            preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "        else:\n",
    "            preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total   += y.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X = X.to(device)\n",
    "            logits = model(X).squeeze(-1)\n",
    "            if cfg.n_classes == 2:\n",
    "                preds = (torch.sigmoid(logits) > 0.5).long().cpu()\n",
    "            else:\n",
    "                preds = logits.argmax(dim=-1).cpu()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(y.tolist())\n",
    "    avg = 'binary' if cfg.n_classes == 2 else 'macro'\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1  = f1_score(all_labels, all_preds, average=avg)\n",
    "    return acc, f1\n",
    "\n",
    "print('Training/eval functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Experiment Loop: Run All 5 PE Methods ─────────────────────────────────\n",
    "PE_METHODS = {\n",
    "    'sinusoidal': SinusoidalPositionalEncoding,\n",
    "    'binary':     BinaryPositionalEncoding,\n",
    "    'rope':       RoPEPositionalEncoding,\n",
    "    'learned':    LearnedPositionalEncoding,\n",
    "    'dape':       DAPEPositionalEncoding,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "criterion = nn.BCEWithLogitsLoss() if cfg.n_classes == 2 else nn.CrossEntropyLoss()\n",
    "\n",
    "for pe_name, pe_class in PE_METHODS.items():\n",
    "    print(f'\\n=== {pe_name.upper()} PE ===')\n",
    "    torch.manual_seed(cfg.seed)  # same init for fair comparison\n",
    "\n",
    "    model = TransformerClassifier(\n",
    "        vocab_size=len(vocab), n_classes=cfg.n_classes,\n",
    "        d_model=cfg.d_model, n_heads=cfg.n_heads,\n",
    "        n_layers=cfg.n_layers, d_ff=cfg.d_ff,\n",
    "        max_seq_len=cfg.max_seq_len, pe_class=pe_class,\n",
    "        dropout=cfg.dropout\n",
    "    ).to(device)\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'  Parameters: {n_params:,}')\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n",
    "\n",
    "    best_acc, best_f1 = 0.0, 0.0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_acc, val_f1 = evaluate(model, val_loader)\n",
    "        scheduler.step()\n",
    "        if val_acc > best_acc:\n",
    "            best_acc, best_f1 = val_acc, val_f1\n",
    "        print(f'  Ep {epoch+1:02d} | loss {train_loss:.4f} | train_acc {train_acc:.4f} | val_acc {val_acc:.4f} | val_f1 {val_f1:.4f}')\n",
    "\n",
    "    elapsed = time.time() - t_start\n",
    "    results[pe_name] = {'accuracy': best_acc, 'f1': best_f1, 'time_s': elapsed, 'params': n_params}\n",
    "    print(f'  Done in {elapsed:.1f}s — best val acc: {best_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Results Table ─────────────────────────────────────────────────────────\n",
    "print('\\n' + '='*65)\n",
    "print(f'PHASE 1 RESULTS — SST-2 (SHORT sequences, binary sentiment)')\n",
    "print('='*65)\n",
    "print(f'{\"PE Method\":<15} {\"Accuracy\":>10} {\"F1\":>10} {\"Time (s)\":>10} {\"Params\":>12}')\n",
    "print('-'*65)\n",
    "\n",
    "best_acc_val = max(v['accuracy'] for v in results.values())\n",
    "for pe_name, m in results.items():\n",
    "    marker = ' <-- BEST' if m['accuracy'] == best_acc_val else ''\n",
    "    print(f'{pe_name:<15} {m[\"accuracy\"]:>10.4f} {m[\"f1\"]:>10.4f} {m[\"time_s\"]:>10.1f} {m[\"params\"]:>12,}{marker}')\n",
    "\n",
    "print('='*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Plots ─────────────────────────────────────────────────────────────────\n",
    "names = list(results.keys())\n",
    "accs  = [results[n]['accuracy'] for n in names]\n",
    "f1s   = [results[n]['f1']       for n in names]\n",
    "times = [results[n]['time_s']   for n in names]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "colors = ['#4C72B0', '#DD8452', '#55A868', '#C44E52', '#8172B2']\n",
    "\n",
    "axes[0].bar(names, accs, color=colors)\n",
    "axes[0].set_title('Accuracy — SST-2')\n",
    "axes[0].set_ylim(min(accs)*0.98, 1.0)\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "axes[1].bar(names, f1s, color=colors)\n",
    "axes[1].set_title('F1 Score — SST-2')\n",
    "axes[1].set_ylim(min(f1s)*0.98, 1.0)\n",
    "axes[1].set_ylabel('F1')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "axes[2].bar(names, times, color=colors)\n",
    "axes[2].set_title('Training Time (s) — SST-2')\n",
    "axes[2].set_ylabel('Seconds')\n",
    "axes[2].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.suptitle('Phase 1: PE Comparison on SST-2 (SHORT sequences)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results_sst2.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: results_sst2.png')"
   ]
  }
 ]
}
